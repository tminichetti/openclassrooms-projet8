{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement du Modele de Segmentation\n",
    "\n",
    "Ce notebook realise l'entrainement complet du modele U-Net sur le dataset Cityscapes.\n",
    "\n",
    "## Objectifs\n",
    "- Charger les donnees avec les generateurs\n",
    "- Configurer et lancer l'entrainement\n",
    "- Suivre les metriques (loss, accuracy, IoU, Dice)\n",
    "- Sauvegarder le meilleur modele\n",
    "- Visualiser les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping, \n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    "    CSVLogger\n",
    ")\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Configuration GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Memory growth active pour {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Erreur GPU: {e}\")\n",
    "\n",
    "# Seed pour reproductibilite\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = Path('../models')\n",
    "LOGS_DIR = Path('../logs')\n",
    "\n",
    "# Creer les dossiers si necessaire\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "LOGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Charger la configuration\n",
    "with open(DATA_DIR / 'config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "IMG_HEIGHT = config['img_height']\n",
    "IMG_WIDTH = config['img_width']\n",
    "N_CLASSES = config['n_classes']\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "print(f\"Configuration chargee:\")\n",
    "print(f\"  - Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"  - Nombre de classes: {N_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametres d'entrainement\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 10  # Pour early stopping\n",
    "\n",
    "# Nom de l'experience (pour les logs)\n",
    "EXPERIMENT_NAME = f\"unet_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"Hyperparametres:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Patience: {PATIENCE}\")\n",
    "print(f\"  - Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration des Classes et Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories Cityscapes\n",
    "CATEGORIES = {\n",
    "    0: {'name': 'void', 'color': (0, 0, 0)},\n",
    "    1: {'name': 'flat', 'color': (128, 64, 128)},\n",
    "    2: {'name': 'construction', 'color': (70, 70, 70)},\n",
    "    3: {'name': 'object', 'color': (153, 153, 153)},\n",
    "    4: {'name': 'nature', 'color': (107, 142, 35)},\n",
    "    5: {'name': 'sky', 'color': (70, 130, 180)},\n",
    "    6: {'name': 'human', 'color': (220, 20, 60)},\n",
    "    7: {'name': 'vehicle', 'color': (0, 0, 142)}\n",
    "}\n",
    "\n",
    "# Mapping labels -> categories\n",
    "LABEL_TO_CATEGORY = {\n",
    "    0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0,\n",
    "    7: 1, 8: 1, 9: 1, 10: 1,\n",
    "    11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2,\n",
    "    17: 3, 18: 3, 19: 3, 20: 3,\n",
    "    21: 4, 22: 4,\n",
    "    23: 5,\n",
    "    24: 6, 25: 6,\n",
    "    26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7, 32: 7, 33: 7,\n",
    "}\n",
    "\n",
    "# LUT pour conversion rapide\n",
    "LUT = np.zeros(256, dtype=np.uint8)\n",
    "for label_id, cat_id in LABEL_TO_CATEGORY.items():\n",
    "    LUT[label_id] = cat_id\n",
    "\n",
    "print(f\"Categories: {[CATEGORIES[i]['name'] for i in range(N_CLASSES)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    \"\"\"Charge et preprocesse une image.\"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (target_size[1], target_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_label(path, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    \"\"\"Charge et convertit un mask en 8 categories.\"\"\"\n",
    "    mask = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, (target_size[1], target_size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    mask = LUT[mask]\n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_to_onehot(mask, n_classes=N_CLASSES):\n",
    "    \"\"\"Convertit un mask en one-hot.\"\"\"\n",
    "    return np.eye(n_classes, dtype=np.float32)[mask]\n",
    "\n",
    "\n",
    "class DataAugmentation:\n",
    "    \"\"\"Augmentation coherente image + mask.\"\"\"\n",
    "    \n",
    "    def __init__(self, horizontal_flip=True, brightness_range=(0.8, 1.2), \n",
    "                 contrast_range=(0.9, 1.1), rotation_range=5):\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.brightness_range = brightness_range\n",
    "        self.contrast_range = contrast_range\n",
    "        self.rotation_range = rotation_range\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        # Flip horizontal\n",
    "        if self.horizontal_flip and np.random.random() > 0.5:\n",
    "            image = np.fliplr(image)\n",
    "            mask = np.fliplr(mask)\n",
    "        \n",
    "        # Brightness\n",
    "        if self.brightness_range:\n",
    "            factor = np.random.uniform(*self.brightness_range)\n",
    "            image = np.clip(image * factor, 0, 1)\n",
    "        \n",
    "        # Contrast\n",
    "        if self.contrast_range:\n",
    "            factor = np.random.uniform(*self.contrast_range)\n",
    "            mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
    "            image = np.clip((image - mean) * factor + mean, 0, 1)\n",
    "        \n",
    "        return image.astype(np.float32), mask\n",
    "\n",
    "\n",
    "class CityscapesGenerator(Sequence):\n",
    "    \"\"\"Generateur de donnees Keras pour Cityscapes.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, batch_size=8, target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                 n_classes=N_CLASSES, augmentation=None, shuffle=True):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.n_classes = n_classes\n",
    "        self.augmentation = augmentation\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "        \n",
    "        for idx in batch_indexes:\n",
    "            sample = self.data[idx]\n",
    "            \n",
    "            image = load_image(sample['image'], self.target_size)\n",
    "            mask = load_label(sample['label'], self.target_size)\n",
    "            \n",
    "            if self.augmentation:\n",
    "                image, mask = self.augmentation(image, mask)\n",
    "            \n",
    "            batch_images.append(image)\n",
    "            batch_masks.append(mask_to_onehot(mask, self.n_classes))\n",
    "        \n",
    "        return np.array(batch_images), np.array(batch_masks)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "print(\"Data generator defini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les chemins des donnees\n",
    "train_df = pd.read_csv(DATA_DIR / 'train_paths.csv')\n",
    "val_df = pd.read_csv(DATA_DIR / 'val_paths.csv')\n",
    "\n",
    "train_data = train_df.to_dict('records')\n",
    "val_data = val_df.to_dict('records')\n",
    "\n",
    "print(f\"Train: {len(train_data)} images\")\n",
    "print(f\"Val: {len(val_data)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer les generateurs\n",
    "train_augmentation = DataAugmentation(\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    contrast_range=(0.9, 1.1)\n",
    ")\n",
    "\n",
    "train_generator = CityscapesGenerator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augmentation=train_augmentation,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = CityscapesGenerator(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augmentation=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_generator)}\")\n",
    "print(f\"Val batches: {len(val_generator)}\")\n",
    "\n",
    "# Verifier un batch\n",
    "X_batch, y_batch = train_generator[0]\n",
    "print(f\"\\nBatch shapes: X={X_batch.shape}, y={y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definition du Modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, n_filters, kernel_size=3, batch_norm=True):\n",
    "    \"\"\"Bloc de convolution double.\"\"\"\n",
    "    x = layers.Conv2D(n_filters, kernel_size, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(n_filters, kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
    "    if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def encoder_block(inputs, n_filters, dropout_rate=0.3):\n",
    "    \"\"\"Bloc encodeur.\"\"\"\n",
    "    skip = conv_block(inputs, n_filters)\n",
    "    pool = layers.MaxPooling2D((2, 2))(skip)\n",
    "    pool = layers.Dropout(dropout_rate)(pool)\n",
    "    return skip, pool\n",
    "\n",
    "\n",
    "def decoder_block(inputs, skip, n_filters, dropout_rate=0.3):\n",
    "    \"\"\"Bloc decodeur.\"\"\"\n",
    "    x = layers.Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(inputs)\n",
    "    x = layers.Concatenate()([x, skip])\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = conv_block(x, n_filters)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet(input_shape, n_classes, filters=[32, 64, 128, 256, 512]):\n",
    "    \"\"\"Construit un modele U-Net.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encodeur\n",
    "    skip1, pool1 = encoder_block(inputs, filters[0])\n",
    "    skip2, pool2 = encoder_block(pool1, filters[1])\n",
    "    skip3, pool3 = encoder_block(pool2, filters[2])\n",
    "    skip4, pool4 = encoder_block(pool3, filters[3])\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = conv_block(pool4, filters[4])\n",
    "    \n",
    "    # Decodeur\n",
    "    dec4 = decoder_block(bottleneck, skip4, filters[3])\n",
    "    dec3 = decoder_block(dec4, skip3, filters[2])\n",
    "    dec2 = decoder_block(dec3, skip2, filters[1])\n",
    "    dec1 = decoder_block(dec2, skip1, filters[0])\n",
    "    \n",
    "    # Sortie\n",
    "    outputs = layers.Conv2D(n_classes, (1, 1), activation='softmax')(dec1)\n",
    "    \n",
    "    return Model(inputs, outputs, name='UNet')\n",
    "\n",
    "print(\"Architecture U-Net definie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer le modele\n",
    "model = build_unet(INPUT_SHAPE, N_CLASSES)\n",
    "\n",
    "print(f\"Modele: {model.name}\")\n",
    "print(f\"Input shape: {model.input_shape}\")\n",
    "print(f\"Output shape: {model.output_shape}\")\n",
    "print(f\"Parametres: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fonctions de Perte et Metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Coefficient de Dice.\"\"\"\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (\n",
    "        tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth\n",
    "    )\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"Dice Loss.\"\"\"\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"Categorical Cross-Entropy + Dice Loss.\"\"\"\n",
    "    cce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return cce + dice\n",
    "\n",
    "\n",
    "class MeanIoU(tf.keras.metrics.Metric):\n",
    "    \"\"\"Mean Intersection over Union.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, name='mean_iou', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.total_iou = self.add_weight(name='total_iou', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_labels = tf.argmax(y_true, axis=-1)\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        iou_per_class = []\n",
    "        for c in range(self.num_classes):\n",
    "            true_c = tf.cast(tf.equal(y_true_labels, c), tf.float32)\n",
    "            pred_c = tf.cast(tf.equal(y_pred_labels, c), tf.float32)\n",
    "            intersection = tf.reduce_sum(true_c * pred_c)\n",
    "            union = tf.reduce_sum(true_c) + tf.reduce_sum(pred_c) - intersection\n",
    "            iou = tf.where(union > 0, intersection / union, 0.0)\n",
    "            iou_per_class.append(iou)\n",
    "        \n",
    "        mean_iou = tf.reduce_mean(tf.stack(iou_per_class))\n",
    "        self.total_iou.assign_add(mean_iou)\n",
    "        self.count.assign_add(1.0)\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total_iou / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total_iou.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "\n",
    "print(\"Loss et metriques definies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compilation du Modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modele\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=combined_loss,\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        dice_coefficient,\n",
    "        MeanIoU(num_classes=N_CLASSES)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Modele compile avec:\")\n",
    "print(f\"  - Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"  - Loss: Combined (CCE + Dice)\")\n",
    "print(f\"  - Metriques: accuracy, dice, mIoU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configuration des Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dossier pour cette experience\n",
    "exp_dir = LOGS_DIR / EXPERIMENT_NAME\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    # Sauvegarder le meilleur modele\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(MODELS_DIR / 'unet_best.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Sauvegarder a chaque epoch\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(MODELS_DIR / 'unet_latest.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=False,\n",
    "        verbose=0\n",
    "    ),\n",
    "    \n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduire le learning rate\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=PATIENCE // 2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard\n",
    "    TensorBoard(\n",
    "        log_dir=str(exp_dir),\n",
    "        histogram_freq=1,\n",
    "        write_graph=True\n",
    "    ),\n",
    "    \n",
    "    # CSV Logger\n",
    "    CSVLogger(\n",
    "        filename=str(exp_dir / 'training_log.csv'),\n",
    "        separator=',',\n",
    "        append=False\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Callbacks configures:\")\n",
    "print(f\"  - ModelCheckpoint (best + latest)\")\n",
    "print(f\"  - EarlyStopping (patience={PATIENCE})\")\n",
    "print(f\"  - ReduceLROnPlateau\")\n",
    "print(f\"  - TensorBoard (logs: {exp_dir})\")\n",
    "print(f\"  - CSVLogger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder la configuration de l'experience\n",
    "experiment_config = {\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'model': 'UNet',\n",
    "    'input_shape': INPUT_SHAPE,\n",
    "    'n_classes': N_CLASSES,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'patience': PATIENCE,\n",
    "    'loss': 'combined_loss',\n",
    "    'optimizer': 'Adam',\n",
    "    'train_samples': len(train_data),\n",
    "    'val_samples': len(val_data),\n",
    "    'augmentation': {\n",
    "        'horizontal_flip': True,\n",
    "        'brightness_range': [0.8, 1.2],\n",
    "        'contrast_range': [0.9, 1.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(exp_dir / 'config.json', 'w') as f:\n",
    "    json.dump(experiment_config, f, indent=2)\n",
    "\n",
    "print(\"Configuration de l'experience sauvegardee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEBUT DE L'ENTRAINEMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nExperience: {EXPERIMENT_NAME}\")\n",
    "print(f\"Train: {len(train_data)} images ({len(train_generator)} batches)\")\n",
    "print(f\"Val: {len(val_data)} images ({len(val_generator)} batches)\")\n",
    "print(f\"\\nLancement...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTRAINEMENT TERMINE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisation des Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Visualise les courbes d'apprentissage.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss (Combined)', fontsize=12)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[0, 1].set_title('Accuracy', fontsize=12)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Dice coefficient\n",
    "    axes[1, 0].plot(history.history['dice_coefficient'], label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(history.history['val_dice_coefficient'], label='Validation', linewidth=2)\n",
    "    axes[1, 0].set_title('Dice Coefficient', fontsize=12)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Dice')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mean IoU\n",
    "    axes[1, 1].plot(history.history['mean_iou'], label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(history.history['val_mean_iou'], label='Validation', linewidth=2)\n",
    "    axes[1, 1].set_title('Mean IoU', fontsize=12)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('mIoU')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Figure sauvegardee: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualiser\n",
    "plot_training_history(history, save_path=exp_dir / 'training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume des resultats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUME DES RESULTATS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Meilleurs scores\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "print(f\"\\nMeilleur epoch: {best_epoch + 1}\")\n",
    "print(f\"\\nScores sur validation:\")\n",
    "print(f\"  - Loss: {history.history['val_loss'][best_epoch]:.4f}\")\n",
    "print(f\"  - Accuracy: {history.history['val_accuracy'][best_epoch]:.4f}\")\n",
    "print(f\"  - Dice: {history.history['val_dice_coefficient'][best_epoch]:.4f}\")\n",
    "print(f\"  - mIoU: {history.history['val_mean_iou'][best_epoch]:.4f}\")\n",
    "\n",
    "print(f\"\\nScores finaux:\")\n",
    "print(f\"  - Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  - Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"  - Dice: {history.history['val_dice_coefficient'][-1]:.4f}\")\n",
    "print(f\"  - mIoU: {history.history['val_mean_iou'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predictions sur Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, generator, n_samples=4):\n",
    "    \"\"\"Visualise des predictions du modele.\"\"\"\n",
    "    X, y_true = generator[0]\n",
    "    y_pred = model.predict(X[:n_samples], verbose=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Image originale\n",
    "        axes[i, 0].imshow(X[i])\n",
    "        axes[i, 0].set_title('Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Mask reel\n",
    "        true_mask = np.argmax(y_true[i], axis=-1)\n",
    "        axes[i, 1].imshow(true_mask, cmap='tab10', vmin=0, vmax=N_CLASSES-1)\n",
    "        axes[i, 1].set_title('Mask Reel')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Mask predit\n",
    "        pred_mask = np.argmax(y_pred[i], axis=-1)\n",
    "        axes[i, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=N_CLASSES-1)\n",
    "        axes[i, 2].set_title('Mask Predit')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Overlay prediction sur image\n",
    "        overlay = X[i].copy()\n",
    "        for cat_id, info in CATEGORIES.items():\n",
    "            mask_cat = pred_mask == cat_id\n",
    "            overlay[mask_cat] = overlay[mask_cat] * 0.5 + np.array(info['color'])/255.0 * 0.5\n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(exp_dir / 'predictions_sample.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, val_generator, n_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder l'historique d'entrainement\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(exp_dir / 'history.csv', index=False)\n",
    "print(f\"Historique sauvegarde: {exp_dir / 'history.csv'}\")\n",
    "\n",
    "# Sauvegarder le modele final\n",
    "model.save(MODELS_DIR / 'unet_final.keras')\n",
    "print(f\"Modele final sauvegarde: {MODELS_DIR / 'unet_final.keras'}\")\n",
    "\n",
    "# Resume des fichiers crees\n",
    "print(f\"\\nFichiers crees:\")\n",
    "print(f\"  - {MODELS_DIR / 'unet_best.keras'}\")\n",
    "print(f\"  - {MODELS_DIR / 'unet_final.keras'}\")\n",
    "print(f\"  - {exp_dir / 'training_log.csv'}\")\n",
    "print(f\"  - {exp_dir / 'history.csv'}\")\n",
    "print(f\"  - {exp_dir / 'training_curves.png'}\")\n",
    "print(f\"  - {exp_dir / 'predictions_sample.png'}\")\n",
    "print(f\"  - {exp_dir / 'config.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTRAINEMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModele: {model.name}\")\n",
    "print(f\"Parametres: {model.count_params():,}\")\n",
    "print(f\"\\nMeilleurs scores (validation):\")\n",
    "print(f\"  - Dice: {max(history.history['val_dice_coefficient']):.4f}\")\n",
    "print(f\"  - mIoU: {max(history.history['val_mean_iou']):.4f}\")\n",
    "print(f\"\\nProchaine etape: Notebook 05 - Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes pour ameliorer les resultats\n",
    "\n",
    "1. **Augmenter les epochs** si le modele continue a s'ameliorer\n",
    "2. **Utiliser un backbone pre-entraine** (VGG16, ResNet) pour de meilleurs resultats\n",
    "3. **Augmentation plus agressive** (rotation, zoom, etc.)\n",
    "4. **Ajuster le learning rate** avec un scheduler\n",
    "5. **Utiliser une taille d'image plus grande** si la memoire le permet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
