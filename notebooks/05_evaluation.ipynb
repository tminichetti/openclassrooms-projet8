{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √âvaluation et Comparaison des Mod√®les\n",
    "\n",
    "Ce notebook r√©alise l'√©valuation compl√®te et la comparaison des mod√®les entra√Æn√©s (bas√©e sur le validation set).\n",
    "\n",
    "## Objectifs\n",
    "- Charger et comparer tous les runs d'entra√Ænement disponibles\n",
    "- Analyser l'impact de la data augmentation quand les paires existent\n",
    "- Comparer les architectures (U-Net vs VGG16) si pr√©sentes\n",
    "- G√©n√©rer un tableau comparatif fiable pour la note technique\n",
    "- Visualiser des pr√©dictions qualitatives via les logs d'entra√Ænement\n",
    "- Identifier le meilleur mod√®le selon Dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "LOGS_DIR = Path('../logs')\n",
    "MODELS_DIR = Path('../models')\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "with open(DATA_DIR / 'config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "IMG_HEIGHT = config['img_height']\n",
    "IMG_WIDTH = config['img_width']\n",
    "N_CLASSES = config['n_classes']\n",
    "\n",
    "print(f\"Configuration: {IMG_WIDTH}x{IMG_HEIGHT}, {N_CLASSES} classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des R√©sultats d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et valider tous les r√©sultats\n",
    "results_file = LOGS_DIR / 'all_results.csv'\n",
    "required_cols = [\n",
    "    'experiment', 'model', 'augmentation', 'epochs_trained', 'best_epoch',\n",
    "    'training_time_minutes', 'val_loss', 'val_accuracy', 'val_dice', 'val_miou',\n",
    "    'model_path', 'timestamp'\n",
    "]\n",
    "\n",
    "def parse_bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip().lower()\n",
    "    if s in {'true', '1', 'yes', 'oui'}:\n",
    "        return True\n",
    "    if s in {'false', '0', 'no', 'non'}:\n",
    "        return False\n",
    "    return np.nan\n",
    "\n",
    "if not results_file.exists():\n",
    "    print('‚ùå Aucun r√©sultat trouv√© !')\n",
    "    df_results_raw = pd.DataFrame(columns=required_cols)\n",
    "else:\n",
    "    df_results_raw = pd.read_csv(results_file)\n",
    "    print(f\"üì• {len(df_results_raw)} ligne(s) brute(s) charg√©e(s) depuis {results_file}\")\n",
    "\n",
    "missing_cols = [c for c in required_cols if c not in df_results_raw.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è Colonnes manquantes ajout√©es avec NaN: {missing_cols}\")\n",
    "    for col in missing_cols:\n",
    "        df_results_raw[col] = np.nan\n",
    "\n",
    "df_results_raw = df_results_raw[required_cols].copy()\n",
    "\n",
    "df_results_raw['augmentation'] = df_results_raw['augmentation'].apply(parse_bool)\n",
    "for col in ['epochs_trained', 'best_epoch', 'training_time_minutes', 'val_loss', 'val_accuracy', 'val_dice', 'val_miou']:\n",
    "    df_results_raw[col] = pd.to_numeric(df_results_raw[col], errors='coerce')\n",
    "df_results_raw['model'] = df_results_raw['model'].astype(str).str.lower().str.strip()\n",
    "\n",
    "invalid_mask = (\n",
    "    df_results_raw['model'].eq('')\n",
    "    | df_results_raw['augmentation'].isna()\n",
    "    | df_results_raw['val_dice'].isna()\n",
    "    | df_results_raw['training_time_minutes'].isna()\n",
    "    | (df_results_raw['training_time_minutes'] <= 0)\n",
    ")\n",
    "df_invalid = df_results_raw[invalid_mask].copy()\n",
    "df_results_valid = df_results_raw[~invalid_mask].copy()\n",
    "\n",
    "print(f\"‚úÖ Runs valides: {len(df_results_valid)}\")\n",
    "print(f\"‚ö†Ô∏è Runs incoh√©rents filtr√©s: {len(df_invalid)}\")\n",
    "if len(df_invalid) > 0:\n",
    "    display(df_invalid[['experiment', 'model', 'augmentation', 'training_time_minutes', 'val_dice']])\n",
    "\n",
    "if len(df_results_valid) > 0:\n",
    "    print('\\nAper√ßu des runs valides:')\n",
    "    display(df_results_valid[['experiment', 'model', 'augmentation', 'val_dice', 'val_miou', 'val_accuracy', 'training_time_minutes', 'epochs_trained']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tableau Comparatif des Mod√®les\n",
    "\n",
    "### 3.1 Vue d'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table principale: meilleur run par couple (model, augmentation)\n",
    "if len(df_results_valid) == 0:\n",
    "    df_best = pd.DataFrame(columns=df_results_valid.columns)\n",
    "    print('‚ùå Aucune donn√©e valide pour la comparaison')\n",
    "else:\n",
    "    idx = df_results_valid.groupby(['model', 'augmentation'])['val_dice'].idxmax()\n",
    "    df_best = df_results_valid.loc[idx].copy().sort_values(['model', 'augmentation']).reset_index(drop=True)\n",
    "    print(f\"‚úÖ {len(df_best)} sc√©nario(s) retenu(s) (meilleur run par couple mod√®le/augmentation)\")\n",
    "\n",
    "if len(df_best) > 0:\n",
    "    df_table = df_best[['model', 'augmentation', 'val_dice', 'val_miou', 'val_accuracy', 'training_time_minutes', 'epochs_trained']].copy()\n",
    "    df_table.columns = ['Mod√®le', 'Augmentation', 'Dice', 'mIoU', 'Accuracy', 'Temps (min)', 'Epochs']\n",
    "    df_table['Mod√®le'] = df_table['Mod√®le'].str.upper()\n",
    "    df_table['Augmentation'] = df_table['Augmentation'].map({True: 'Oui', False: 'Non'})\n",
    "\n",
    "    df_display = df_table.copy()\n",
    "    df_display['Dice'] = df_display['Dice'].map(lambda x: f'{x:.4f}')\n",
    "    df_display['mIoU'] = df_display['mIoU'].map(lambda x: f'{x:.4f}')\n",
    "    df_display['Accuracy'] = df_display['Accuracy'].map(lambda x: f'{x:.4f}')\n",
    "    df_display['Temps (min)'] = df_display['Temps (min)'].map(lambda x: f'{x:.1f}')\n",
    "\n",
    "    print('\\n' + '='*80)\n",
    "    print('TABLEAU COMPARATIF DES MOD√àLES (RUNS VALIDES)')\n",
    "    print('='*80 + '\\n')\n",
    "    display(df_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tableau pour export (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter en LaTeX pour la note technique\n",
    "if len(df_best) == 0:\n",
    "    print('‚ö†Ô∏è Export LaTeX non g√©n√©r√©: aucune donn√©e valide')\n",
    "else:\n",
    "    df_table_export = df_best[['model', 'augmentation', 'val_dice', 'val_miou', 'val_accuracy', 'training_time_minutes', 'epochs_trained']].copy()\n",
    "    df_table_export.columns = ['Mod√®le', 'Augmentation', 'Dice', 'mIoU', 'Accuracy', 'Temps (min)', 'Epochs']\n",
    "    df_table_export['Mod√®le'] = df_table_export['Mod√®le'].str.upper()\n",
    "    df_table_export['Augmentation'] = df_table_export['Augmentation'].map({True: 'Oui', False: 'Non'})\n",
    "\n",
    "    latex_table = df_table_export.to_latex(\n",
    "        index=False,\n",
    "        float_format='%.4f',\n",
    "        caption='Comparaison des performances des mod√®les de segmentation s√©mantique',\n",
    "        label='tab:model_comparison'\n",
    "    )\n",
    "    with open(LOGS_DIR / 'comparison_table.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print('‚úÖ Tableau LaTeX sauvegard√©: logs/comparison_table.tex\\n')\n",
    "    print(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisations Comparatives\n",
    "\n",
    "### 4.1 Graphiques de comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations comparatives\n",
    "if len(df_best) == 0:\n",
    "    print('‚ö†Ô∏è Graphiques de comparaison non g√©n√©r√©s: aucune donn√©e valide')\n",
    "else:\n",
    "    df_plot = df_best.copy()\n",
    "    df_plot['label'] = df_plot.apply(lambda x: f\"{x['model'].upper()}\\n{'avec aug' if x['augmentation'] else 'sans aug'}\", axis=1)\n",
    "    colors = ['#2ecc71' if aug else '#e74c3c' for aug in df_plot['augmentation']]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    axes[0, 0].bar(range(len(df_plot)), df_plot['val_dice'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[0, 0].set_xticks(range(len(df_plot)))\n",
    "    axes[0, 0].set_xticklabels(df_plot['label'], fontsize=9)\n",
    "    axes[0, 0].set_title('Dice Coefficient', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Dice')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    for i, v in enumerate(df_plot['val_dice']):\n",
    "        axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "    axes[0, 1].bar(range(len(df_plot)), df_plot['val_miou'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[0, 1].set_xticks(range(len(df_plot)))\n",
    "    axes[0, 1].set_xticklabels(df_plot['label'], fontsize=9)\n",
    "    axes[0, 1].set_title('Mean IoU (Jaccard)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('mIoU')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "    for i, v in enumerate(df_plot['val_miou']):\n",
    "        axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "    axes[1, 0].bar(range(len(df_plot)), df_plot['val_accuracy'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[1, 0].set_xticks(range(len(df_plot)))\n",
    "    axes[1, 0].set_xticklabels(df_plot['label'], fontsize=9)\n",
    "    axes[1, 0].set_title('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "    for i, v in enumerate(df_plot['val_accuracy']):\n",
    "        axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "    axes[1, 1].bar(range(len(df_plot)), df_plot['training_time_minutes'], color='#3498db', edgecolor='black', linewidth=1.5)\n",
    "    axes[1, 1].set_xticks(range(len(df_plot)))\n",
    "    axes[1, 1].set_xticklabels(df_plot['label'], fontsize=9)\n",
    "    axes[1, 1].set_title(\"Temps d'entra√Ænement\", fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Minutes')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    time_max = max(float(df_plot['training_time_minutes'].max()), 1.0)\n",
    "    for i, v in enumerate(df_plot['training_time_minutes']):\n",
    "        axes[1, 1].text(i, v + (time_max * 0.02), f'{v:.0f}min', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#2ecc71', edgecolor='black', label='Avec augmentation'),\n",
    "        Patch(facecolor='#e74c3c', edgecolor='black', label='Sans augmentation')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.98), ncol=2, fontsize=11)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(LOGS_DIR / 'comparison_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    print('‚úÖ Graphique sauvegard√©: logs/comparison_metrics.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparaison directe Dice vs mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "if len(df_best) == 0:\n",
    "    print('‚ö†Ô∏è Graphique Dice vs mIoU non g√©n√©r√©: aucune donn√©e valide')\n",
    "else:\n",
    "    for _, row in df_best.iterrows():\n",
    "        marker = 'o' if row['augmentation'] else 's'\n",
    "        color = '#2ecc71' if row['augmentation'] else '#e74c3c'\n",
    "        label = f\"{row['model'].upper()} ({'aug' if row['augmentation'] else 'no-aug'})\"\n",
    "        ax.scatter(row['val_dice'], row['val_miou'], s=300, marker=marker, color=color, edgecolor='black', linewidth=2, label=label, alpha=0.7)\n",
    "        ax.annotate(row['model'].upper(), (row['val_dice'], row['val_miou']), xytext=(10, 10), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('Dice Coefficient', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Mean IoU', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Comparaison Dice vs mIoU', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    lims = [0, 1]\n",
    "    ax.plot(lims, lims, 'k--', alpha=0.3, linewidth=2, label='Dice = mIoU')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(LOGS_DIR / 'dice_vs_miou.png', dpi=150, bbox_inches='tight')\n",
    "    print('‚úÖ Graphique sauvegard√©: logs/dice_vs_miou.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse de l'Impact de l'Augmentation\n",
    "\n",
    "### 5.1 Calcul des gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print(\"ANALYSE DE L'IMPACT DE L'AUGMENTATION DE DONN√âES\")\n",
    "print('='*80 + '\\n')\n",
    "\n",
    "gains = []\n",
    "\n",
    "if len(df_best) == 0:\n",
    "    print(\"‚ö†Ô∏è Aucune donn√©e valide pour analyser l'augmentation\")\n",
    "else:\n",
    "    for model_name in sorted(df_best['model'].unique()):\n",
    "        model_df = df_best[df_best['model'] == model_name]\n",
    "        with_aug = model_df[model_df['augmentation'] == True]\n",
    "        without_aug = model_df[model_df['augmentation'] == False]\n",
    "        if len(with_aug) == 0 or len(without_aug) == 0:\n",
    "            print(f\"‚ö†Ô∏è  {model_name.upper()}: Comparaison impossible (manque avec/sans augmentation)\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üìä Mod√®le: {model_name.upper()}\")\n",
    "        print('-' * 80)\n",
    "        metrics = ['val_dice', 'val_miou', 'val_accuracy']\n",
    "        metric_names = ['Dice Coefficient', 'Mean IoU', 'Accuracy']\n",
    "        for metric, name in zip(metrics, metric_names):\n",
    "            val_with = float(with_aug[metric].values[0])\n",
    "            val_without = float(without_aug[metric].values[0])\n",
    "            gain_abs = val_with - val_without\n",
    "            gain_pct = (gain_abs / val_without) * 100 if val_without != 0 else np.nan\n",
    "            print(f\"  {name:20} | Sans aug: {val_without:.4f} | Avec aug: {val_with:.4f} | Gain: {gain_abs:+.4f} ({gain_pct:+.2f}%)\")\n",
    "            gains.append({\n",
    "                'model': model_name,\n",
    "                'metric': name,\n",
    "                'without_aug': val_without,\n",
    "                'with_aug': val_with,\n",
    "                'gain_abs': gain_abs,\n",
    "                'gain_pct': gain_pct\n",
    "            })\n",
    "        print()\n",
    "\n",
    "df_gains = pd.DataFrame(gains)\n",
    "if len(df_gains) > 0:\n",
    "    print('\\nüìà R√©sum√© des gains moyens:')\n",
    "    print('-' * 80)\n",
    "    summary = df_gains.groupby('metric')[['gain_abs', 'gain_pct']].mean()\n",
    "    print(summary.to_string())\n",
    "else:\n",
    "    print('‚ÑπÔ∏è Aucun gain calculable (paires avec/sans augmentation absentes)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualisation des gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_gains) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    pivot_abs = df_gains.pivot(index='metric', columns='model', values='gain_abs')\n",
    "    pivot_abs.plot(kind='bar', ax=axes[0], edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_title('Gains Absolus avec Augmentation', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Gain')\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].legend(title='Mod√®le')\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    pivot_pct = df_gains.pivot(index='metric', columns='model', values='gain_pct')\n",
    "    pivot_pct.plot(kind='bar', ax=axes[1], edgecolor='black', linewidth=1.5)\n",
    "    axes[1].set_title('Gains Relatifs avec Augmentation', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Gain (%)')\n",
    "    axes[1].set_xlabel('')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1].legend(title='Mod√®le')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(LOGS_DIR / 'augmentation_impact.png', dpi=150, bbox_inches='tight')\n",
    "    print('‚úÖ Graphique sauvegard√©: logs/augmentation_impact.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('‚ö†Ô∏è augmentation_impact.png non g√©n√©r√©: gains indisponibles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identification du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner le meilleur mod√®le selon Dice\n",
    "if len(df_best) == 0:\n",
    "    best_model = None\n",
    "    print('‚ö†Ô∏è Aucun meilleur mod√®le identifiable (pas de run valide)')\n",
    "else:\n",
    "    best_idx = df_best['val_dice'].idxmax()\n",
    "    best_model = df_best.loc[best_idx]\n",
    "    print('\\n' + '='*80)\n",
    "    print('üèÜ MEILLEUR MOD√àLE IDENTIFI√â')\n",
    "    print('='*80 + '\\n')\n",
    "    print(f\"Mod√®le: {best_model['model'].upper()}\")\n",
    "    print(f\"Augmentation: {'Oui' if bool(best_model['augmentation']) else 'Non'}\")\n",
    "    print('\\nüìä Performances:')\n",
    "    print(f\"  - Dice Coefficient: {best_model['val_dice']:.4f}\")\n",
    "    print(f\"  - Mean IoU: {best_model['val_miou']:.4f}\")\n",
    "    print(f\"  - Accuracy: {best_model['val_accuracy']:.4f}\")\n",
    "    print('\\n‚è±Ô∏è  Entra√Ænement:')\n",
    "    print(f\"  - Temps: {best_model['training_time_minutes']:.1f} minutes\")\n",
    "    print(f\"  - Epochs: {int(best_model['epochs_trained'])}\")\n",
    "    print('\\nüíæ Fichier mod√®le:')\n",
    "    print(f\"  {best_model['model_path']}\")\n",
    "    print('\\nüìù Exp√©rience:')\n",
    "    print(f\"  {best_model['experiment']}\")\n",
    "    print('\\n' + '='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse des Courbes d'Apprentissage\n",
    "\n",
    "### 7.1 Comparaison des courbes de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "if len(df_best) == 0:\n",
    "    print(\"‚ö†Ô∏è Courbes d'apprentissage non g√©n√©r√©es: aucune donn√©e valide\")\n",
    "else:\n",
    "    has_curve = False\n",
    "    for _, row in df_best.iterrows():\n",
    "        exp_dir = LOGS_DIR / row['experiment']\n",
    "        history_file = exp_dir / 'history.csv'\n",
    "        if not history_file.exists():\n",
    "            print(f\"‚ö†Ô∏è Historique manquant pour {row['experiment']}\")\n",
    "            continue\n",
    "        history = pd.read_csv(history_file)\n",
    "        label = f\"{row['model'].upper()} ({'aug' if row['augmentation'] else 'no-aug'})\"\n",
    "        axes[0, 0].plot(history['val_loss'], label=label, linewidth=2)\n",
    "        axes[0, 1].plot(history['val_dice_coefficient'], label=label, linewidth=2)\n",
    "        axes[1, 0].plot(history['val_mean_iou'], label=label, linewidth=2)\n",
    "        axes[1, 1].plot(history['val_accuracy'], label=label, linewidth=2)\n",
    "        has_curve = True\n",
    "\n",
    "    if has_curve:\n",
    "        axes[0, 0].set_title('Validation Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[0, 1].set_title('Validation Dice', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Dice')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1, 0].set_title('Validation mIoU', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('mIoU')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1, 1].set_title('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(LOGS_DIR / 'learning_curves_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        print('‚úÖ Graphique sauvegard√©: logs/learning_curves_comparison.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('‚ö†Ô∏è learning_curves_comparison.png non g√©n√©r√©: history.csv introuvable')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. R√©capitulatif et Recommandations\n",
    "\n",
    "### 8.1 Synth√®se des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('SYNTH√àSE DES R√âSULTATS')\n",
    "print('='*80 + '\\n')\n",
    "\n",
    "if len(df_best) == 0:\n",
    "    print('‚ùå Aucun sc√©nario valide √† synth√©tiser')\n",
    "else:\n",
    "    print('üìå Sc√©narios retenus (meilleur run par couple mod√®le/augmentation):')\n",
    "    for _, row in df_best.iterrows():\n",
    "        print(f\"  - {row['model'].upper()} ({'avec' if row['augmentation'] else 'sans'} augmentation)\")\n",
    "\n",
    "    print(f\"\\nüèÜ Meilleur mod√®le: {best_model['model'].upper()} ({'avec' if best_model['augmentation'] else 'sans'} augmentation)\")\n",
    "    print(f\"  Dice: {best_model['val_dice']:.4f} | mIoU: {best_model['val_miou']:.4f}\")\n",
    "\n",
    "    if len(df_gains) > 0:\n",
    "        avg_gain = df_gains[df_gains['metric'] == 'Dice Coefficient']['gain_pct'].mean()\n",
    "        print(f\"\\nüìà Gain moyen avec augmentation (Dice): {avg_gain:+.2f}%\")\n",
    "    else:\n",
    "        print('\\nüìà Gain augmentation: non calculable (paires incompl√®tes)')\n",
    "\n",
    "    print('\\nüí° Observations conditionnelles:')\n",
    "    if len(df_gains) > 0:\n",
    "        print(\"  - L'impact de l'augmentation est calcul√© √† partir des paires disponibles.\")\n",
    "    else:\n",
    "        print(\"  - Impossible de conclure sur l'augmentation sans paires avec/sans augmentation.\")\n",
    "\n",
    "    models_present = set(df_best['model'].tolist())\n",
    "    if {'unet', 'vgg16'}.issubset(models_present):\n",
    "        print('  - La comparaison UNet vs VGG16 est disponible pour les sc√©narios pr√©sents.')\n",
    "    else:\n",
    "        print('  - Comparaison UNet vs VGG16 partielle/incompl√®te selon les runs disponibles.')\n",
    "\n",
    "    print('\\nüìã Prochaines √©tapes:')\n",
    "    print(\"  1. Copier le meilleur mod√®le dans l'API:\")\n",
    "    print(f\"     cp {best_model['model_path']} ../api/model/segmentation_model.h5\")\n",
    "    print(\"  2. Tester l'API localement\")\n",
    "    print('  3. D√©ployer API + Streamlit')\n",
    "    print('  4. Int√©grer ces r√©sultats dans la note technique')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export des R√©sultats\n",
    "\n",
    "### 9.1 Sauvegarder tous les graphiques et tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nüìÇ Fichiers g√©n√©r√©s pour la note technique:\\n')\n",
    "\n",
    "files = [\n",
    "    'comparison_table.tex',\n",
    "    'comparison_metrics.png',\n",
    "    'dice_vs_miou.png',\n",
    "    'augmentation_impact.png',\n",
    "    'learning_curves_comparison.png'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    path = LOGS_DIR / file\n",
    "    if path.exists():\n",
    "        print(f\"  ‚úÖ {path}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {path} (non g√©n√©r√© pour ce jeu de donn√©es)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('√âVALUATION TERMIN√âE ‚úÖ')\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cette analyse permet, **si les runs sont pr√©sents**, de :\n",
    "\n",
    "1. Comparer les performances de diff√©rents sc√©narios mod√®le/augmentation\n",
    "2. Quantifier l'impact de l'augmentation uniquement quand les paires existent\n",
    "3. Identifier un meilleur mod√®le selon Dice\n",
    "4. G√©n√©rer tableaux et graphiques pour la note technique\n",
    "\n",
    "Le notebook fonctionne en mode d√©grad√© propre quand certaines combinaisons ne sont pas encore entra√Æn√©es.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}